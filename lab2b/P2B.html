<html>
<head>
<title>Project 2B</title>
</head>
<body>
<center>
<h1>Project 2B<br>
Lock Granularity and Performance</h1>
</center>

<h2>INTRODUCTION:</h2>
<P>
In the previous project the mutex and spin-lock proved to be bottlenecks, 
preventing parallel access to the list. 
In this project, you will do additional performance instrumentation to confirm this, 
and extend your previous solution to deal with this problem. 
This project can be broken up into a few major steps:
<UL>
	<LI>Do performance instrumentation and measurement to confirm the cause of the problem.</li>
	<LI>Implement a new option to divide a list into sublists and support synchronization 
	on sublists, thus allowing parallel access to the (original) list.</li>
	<LI>Do new performance measurements to confirm that the problem has been solved.</li>
</UL>
</p>

<h2>RELATION TO READING AND LECTURES:</h2>
<P>
Partitioned lists and finer granularity locking are discussed in sections 29.2-4
</p>

<h2>PROJECT OBJECTIVES:</h2>
<UL>
	<li>primary: demonstrate the ability to recognize bottlenecks on large data structures</li>
	<li>primary: experience with partitioning a serialized resource to improve parallelism</li>
	<li>primary: experience with basic performance measurement and instrumentation</li>
	<li>primary: experience with execution profiling</li>
	<li>secondary: experience with finding, installing, and exploiting new development tools</li>
</UL>

<h2>DELIVERABLES:</h2>
A single tarball (<tt>.tar.gz</tt>) containing:
<ul>
	<li> <tt>SortedList.h</tt> - a header file containing interfaces for linked list operations.
	     </li>
	<li> <tt>SortedList.c</tt> - the source for a C source module that compiles cleanly 
		(with no errors or warnings), and implements insert, delete, lookup, 
		and length methods for a sorted doubly linked list 
		(described in the provided header file, including correct placement 
		of pthread_yield calls).
		<P>
		You are free to implement methods beyond those described in the
		provided <tt>SortedList.h</tt>, but you cannot make any changes
		to that header file.  This header file describes the interfaces
		that you are required to implement.
		</P>
		</li>
	<li> <tt>lab2_list.c</tt> - the source for a C program that compiles cleanly 
		(with no errors or warnings), and implements the specified command line options 
		(<strong>--threads</strong>, 
		<strong>--iterations</strong>, 
		<strong>--yield</strong>, 
		<strong>--sync</strong>, 
		<strong>--lists</strong>), 
		drives one or more parallel 
		threads that do operations on a shared linked list, and reports on the final list 
		and performance.  
		Note that we expect segmentation faults in non-synchronized multi-thread runs, 
		so your program should catch those and report the run as having failed.
		</li>
	<li>A <tt>Makefile</tt> to build the deliverable programs, output, graphs, and tarball.  
		For your early testing you are free to run your program manually, but by the 
		time you are done, all of the below-described test cases should be executed, 
		the output captured, and the graphs produced automatically.  
		The higher level targets should be:
		<ul>
			<li>default ... the <tt>lab2_list</tt> executable
	     		    (compiling with the <strong><tt>-Wall</tt></strong> and 
			    <strong><tt>-Wextra</tt></strong> options).
			    </li>
			<li><strong>tests</strong> ... run all specified test cases to generate CSV results</li>
			<li><strong>profile</strong> ... run tests with profiling tools to generate an 
				execution profiling report</li>
			<li><strong>graphs</strong> ... use gnuplot to generate the required graphs</li>
			<li><strong>dist</strong> ... create the deliverable tarball</li>
			<li><strong>clean</strong> ... delete all programs and output generated
							by the <tt>Makefile</tt></li>
		</ul></li>
	<li> <tt>lab2b_list.csv</tt> - containing your results for all of test runs.</li>
	<li> <tt>profile.out</tt> - execution profiling report showing where time was spent
		in the un-partitioned spin-lock implementation.  </li>

	<li> graphs (<tt>.png</tt> files), created by <em>gnuplot(1)</em> on the above csv data showing:
		<ul>
			<li> <tt>lab2b_1.png</tt> ... throughput vs. number of threads for mutex and spin-lock 
					synchronized list operations.</li>
			<li> <tt>lab2b_2.png</tt> ... mean time per mutex wait and mean time per operation for 
					mutex-synchronized list operations.</li>
			<li> <tt>lab2b_3.png</tt> ... successful iterations vs. threads for 
					each synchronization method.</li>
			<li> <tt>lab2b_4.png</tt> ... throughput vs. number of threads for mutex 
					synchronized partitioned lists.</li>
			<li> <tt>lab2b_5.png</tt> ... throughput vs. number of threads for 
					spin-lock-synchronized partitioned lists.</li>
		</ul></li>
	<li> any other files or scripts required to generate your results.</li>
	<li> a <tt>README</tt> file containing:
		<ul>
			<li> descriptions of each of the included files and any other information about your 
				submission that you would like to bring to our attention 
	 			(e.g. research, limitations, features, testing methodology).</LI>
			<li> brief (a few sentences per question) answers to each of the questions (below).</li>
		</ul> </li>
</ul>
<h2>PREPARATION:</h2>
<p>
To perform this assignment, you will need to research, choose, install and master a 
multi-threaded execution profiling tool.  Execution profiling is a combination of 
compile-time and run-time tools that analyze a program's execution to determine 
how much time is being spent in which routines.  
There are three standard Linux profiling tools:
<ul>
	<li>The standard Linux <em>gprof(1)</em> tool is quite simple to use, but its 
		call-counting mechanism is not-multi-thread safe, and its execution sampling is 
		not multi-thread aware.  
		As such, it is not usable for analyzing the performance of multi-threaded applications. 
		There are other tools that do solve this problem.  The two best-known are ...</li>
	<li> <em>valgrind</em> ... best known for its memory leak detector, which has an interpreted 
		execution engine that can extract a great deal of information about where 
		cycles are being spent, even estimating cache misses.  
		It does work for multi-threaded programs, but its interpreter does not provide 
		much parallelism.  As such it is not useful for examining high contention situations.</li>
	<li> <em>gperftools</em> ... a wonderful set of performance optimization tools from Google.  
		It includes a profiler that is quite similar to gprof (in that it samples real execution).  
		This is probably the best tool to use for this problem.</li>
</ul>
</p>
<P>
This project is about scalable parallelism, which is only possible on a processor with many cores.  
You can do most of your development and testing on any Linux system, but if your personal computer 
does not have more than a few cores (e.g. 8-12), you will not be able to do real 
multi-threaded performance testing on it.  
Lab servers are available if you need access to a larger machine for your final testing,
graph production and performance analysis.
</p>
<P>
If you are testing on lab servers, you may have to install your own private copies of
the <em>gperftools</em>.  This means that the paths to those tools will be different
on different machines, and greatly complicates creating a <tt>profile</tt> rule that
will work on other machines.  For this reason, we will not run your <tt>profile</tt>
rule during testing.  Rather we will merely review your submitted profiling report
and confirm that it corresponds to your code.
</P>
<h2>PROJECT DESCRIPTION:</h2>
<P>
Review your results from the previous lab (<tt>lab2_add-5.png</tt> and <tt>lab2_list-4.png</tt>)
for the cost (per synchronized operation) vs. the number of threads.
For Compare-and-Swap and Mutex,
we saw that adding threads took us from tens of ns per operation to small hundreds of ns per operation. 
Looking at the analogous results in Part 2, we see the (un-adjusted for length) time per operation go 
from a few microseconds, to tens of microseconds.  
For the adds, moderate contention added ~100ns to each synchronization operation.  
For the list operations, moderate contention added ~10us to each synchronization operation.  
This represents a 100x difference in the per operation price of lock contention.
</P>
<P>
In a single-threaded implementation, time per operation is a very reasonable performance
metric.  But for multi-threaded implemenations we are also concerned about how well
we are taking advantage of the available parallelism.  This is more clearly seen
in the aggregate throughput (total operations per second for all threads combined).
Run your list exerciser with:
<ul>
	<li>Mutex synchronized list operations, 1,000 iterations, 1,2,4,8,12,16,24 threads</li>
	<li>Spin-lock synchronized list operations, 1,000 iterations, 1,2,4,8,12,16,24 threads</li>
</ul>
Capture the output, and produce a plot of the total number of operations per second for each
synchronization method.
In the previous lab, we gave you all of the necessary data reduction scripts.  
In this lab, you will have to create your own ... but you can use the scripts 
provided in the previous lab as a starter.
<ul>
	<li>To get the throughput, divide one billion (number of nanoseconds in a second) 
		by the time per operation (in nanoseconds).  </li>
	<li>Previously, we multiplied the number of operations by half of the mean list
	    length (to account for the longer searches in longer lists).
	    This time, we are focusing on synchronization costs ... and
	    our synchronization is implemented, not per-list-element, but per-operation.
	    Thus, in this analysis, we should use the raw time per operation, 
	    and not correct our times for the list length.</li>
</ul>
Submit this graph as <tt>lab2b_1.png.</tt>
</P>
<P>
You do not need to go back to your lab2a_list program to generate this data, because
the lab2b_list program (even after adding all the new features) should still be able
to generate essentially the same results.
</P>
<P>
The most obvious difference, which we already knew:
<ul>
	<li>Spin-locks waste increasingly more cycles as the probability of contention increases.</li>
</ul>
But these throughput lines show us something that was not as obvious in the cost per operation graphs:
<ul>
	<li>Whereas add throughput quickly leveled off ...
		we had saturated the CPU and the overhead of synchronization 
		seemed to increase only very slowly.</li>
	<li>The list operation throughput continues to drop, as the overhead of 
		synchronization increases with the number of threads - and much worse for spin-locks.</li>
</ul>
</P>
<P>
The obvious conclusions (from both the cost-per-operation graphs you produced previously, and the 
throughput graphs you have just produced) are:
<ul>
	<li>The throughput of parallel synchronized linked list operations does not scale as well as 
the throughput of parallel synchronized add operations.</li>
	<li>The reduction in throughput with increasing parallelism is due to an increasing time per operation.</li>
</ul>
Since the code inside the critical section does not change with the number of threads, it seems reasonable to assume that the added execution time is being spent getting the locks.
</P>
<P>

<ul><strong>
QUESTION 2.3.1 - Cycles in the basic list implementation:
	<ul>
		<p>
		Where do you believe most of the cycles are spent in the 1 and 2-thread list tests ?
		</p><p>
		Why do you believe these to be the most expensive parts of the code?
		</p><p>
		Where do you believe most of the time/cycles are being spent in the high-thread spin-lock tests?
		</p><p>
		Where do you believe most of the time/cycles are being spent in the high-thread mutex tests?
		</p>
	</ul>
</strong></UL>
</P>
<P>
It should be clear why the spin-lock implementation performs so badly with a large number of threads.  
But the mutex implementation should not have this problem.  
You may have some theories about why these algorithms scale so poorly.  
But theories are only theories, and the <em>prime directive</em> of performance analysis is that theoretical
conclusions must be confirmed by hard data.
</p>
<h3>Execution Profiling</h3>
<p>
Build your program with debug symbols, choose your execution profiling package, install it, run the spin-lock list test (1,000 iterations 12 threads) under the profiler, and analyze the results to determine where the cycles are being spent.
</P>
<P>
The default output from <em>google-pprof</em> will show you which routine is consuming most of the cycles.  
If you then re-run <em>google-pprof</em> with the <tt>--list</tt> option (specifying that routine), 
it will give you a source-level breakdown of how much time is being spent on each instruction.  
You should get a very clear answer to the question of where the program is spending its time.  
Update your <tt>Makefile</tt> to run this test and generate the results automatically (<tt>make profile</tt>), 
include your profiling report (in a file named <tt>profile.out</tt>)
in your submitted tarball, and identify it in your <tt>README</tt> file.
</P>
<ul><strong>
QUESTION 2.3.2 - Execution Profiling:
	<ul>
		<p>
		Where (what lines of code) are consuming most of the cycles when the spin-lock version 
		of the list exerciser is run with a large number of threads?
		</p>
		<p>
		Why does this operation become so expensive with large numbers of threads?
		<p>
	</ul>
</strong></UL>
<h3>Timing Mutex Waits</h3>
<P>
In the spin-lock case, profiling can tell us where we are spending most of our time.
In the mutex case, we are not spinning.  A thread that cannot get the lock is blocked, and not consuming any cycles.
Profiling only tells us what code we are executing.  
It doesn't tell us anything about the time we spend blocked.
How could we confirm that, in the mutex case, most threads are spending most of their time waiting for a lock?  
</p>
<P>
Update your mutex and spin-lock implementations to:
<ul>
	<li> Note the high-resolution time before and after getting the lock, 
		compute the elapsed difference, and add that to a (per-thread) total.</li>
	<li> When the program completes, add up the total lock acquisition time 
		(for all threads) and divide it by the number of lock operations to 
		compute an average wait-for-lock, and add this number, 
		as a new (8th) column, to the output statistics for the run.
		If you are not locking, this number should always be zero.</li>
</ul>
</P>
<P>
Run the list mutex test again for 1,000 iterations and 1, 2, 4, 8, 16, 24 threads, 
and plot the wait-for-lock time, and the average time per operation against the 
number of competing threads.  Submit this graph <tt>lab2b_2.png</tt>.  

<ul><strong>
QUESTION 2.3.3 - Mutex Wait Time:<br>
    Look at the average time per operation (vs. # threads) 
    and the average wait-for-mutex time (vs. #threads).
	<ul>
		<li> Why does the average lock-wait time rise so dramatically 
			with the number of contending threads?</li>
		<li> Why does the completion time per operation rise 
			(less dramatically) with the number of contending threads?</li>

		<li> How is it possible for the wait time per operation to go 
			up faster (or higher) than the completion time per operation?</li>
	</ul>
</strong></UL>
</P>
<h3>Addressing the Underlying Problem</h3>
<P>
While the details of how contention degrades throughput are different for mutex and spin-lock synchronization, 
all of the degradation is the result of increased contention.  
This is the fundamental problem with coarse-grained synchronization.  
The classic solution to this problem is to partition the single resource 
(in this case a linked list) into multiple independent resources, and divide the requests among those sub-resources.
</P>
<P>
Add a new <tt>--lists=</tt># option to your lab2_list program:
<ul>
	<li> break the single (huge) sorted list into the specified number of sub-lists (each with its own list header and synchronization object).
	<li> change your lab2_list program to select which sub-list a particular key should be in based on a simple hash of the key, modulo the number of lists.
	<li> figure out how to (safely and correctly) obtain the length of the list, which now involves enumerating all of the sub-lists.
	<li> each thread:
	    <ul>
		<li> starts with a set of pre-allocated and initialized elements 
			(<tt>--iterations=</tt>#)</li>
		<li> inserts them all into the multi-list (which sublist the key 
			should go into determined by a hash of the key)</li>
		<li> gets the list length</li>
		<li> looks up and deletes each of the keys it inserted</li>
		<li> exits to re-join the parent thread</li>
	    </ul></li>
	<li> Include the number of lists as the fourth number 
		(always previously 1) in the output statistics report.</li>
</ul>
<P>
The supported command line options and expected output are illustrated below:
<ul><pre><tt>
% ./lab2_list --threads=10 --iterations=1000 --lists=5 --yield=id --sync=m
list-id-m,10,1000,5,30000,85811144,2860,20580
%
</tt></pre> </ul>
<P>
Confirm the correctness of your new implementation:
<ul>
	<li> Run your program with <tt>--yield=id</tt>, 
		4 lists, 1,4,8,12,16 threads, and 1, 2, 4, 8, 16 iterations 
		(and no synchronization) to see how many iterations it takes 
		to reliably fail (and make sure your <tt>Makefile</tt> 
		expects some of these tests to fail).</li>
	<li> Run your program with <tt>--yield=id</tt>, 4 lists, 
		1,4,8,12,16 threads, and 10, 20, 40, 80 iterations, 
		<tt>--sync=s</tt> and <tt>--sync=m</tt> to confirm that 
		updates are now properly protected
		(i.e., all runs succeeded).</li>
	<li> Plot these results (as you did last week) and submit the 
		result as <tt>lab2b_3.png</tt>.</li>
</ul>
</P>
<P>
Now that we believe the partitioned lists implementation works, we can measure its performance:
<ul>
	<li> Rerun both synchronized versions, without yields, for 1000 iterations, 
		1,2,4,8,12 threads, and 1,4,8,16 lists.</li>
	<li> For each synchronization mechanism, graph the aggregated throughput 
		(total operations per second, as you did for <tt>lab2a_1.png</tt>) 
		vs. the number of threads, with a separate curve for each number of lists
		(1,4,8,16).
		Call these graphs <tt>lab2b_4.png</tt>(symc=m) and 
		<tt>lab2b_5.png</tt>(sync=s).</li>
</ul>
</P>
<P>
<strong><ul>
QUESTION 2.3.4 - Performance of Partitioned Lists
	<ul>
		<li> Explain the change in performance of the synchronized 
			methods as a function of the number of lists.</li>
		<li> Should the throughput continue increasing as the number 
			of lists is further increased?  If not, explain why not.</li>
		<li> It seems reasonable to suggest the throughput of an N-way 
			partitioned list should be equivalent to the throughput of 
			a single list with fewer (1/N) threads.  
			Does this appear to be true in the above curves?  
			If not, explain why not.</li>
	</ul>
</ul></strong>

<H2> SUMMARY OF EXIT CODES: </H2>
<ul>
	<li> 0: successful run</li>
	<li> 1: invalid argument or system call error</li>
	<li> 2: other failures ... e.g. list operation failures due to conflicting updates</li>
</ul>

<h2> SUBMISSION: </h2>
<P>
Your <strong>README</strong> file must include lines of the form:
<ul>
	<strong>NAME:</strong> <em>your name</em><br>
	<strong>EMAIL:</strong> <em>your email</em><br>
	<strong>ID:</strong> <em>your student ID</em>
</ul>
And, if slip days are allowed on this project, and you use some:
<ul>
	<strong>SLIPDAYS:</strong> <em>your student ID</em>,<em>#days</em>
</ul>
Your name, student ID, and email address  should also appear as comments at the top
of your <tt>Makefile</tt> and each source file.
</P>
<p>
Your tarball should have a name of the form <tt>lab2b-</tt><em>studentID</em><tt>.tar.gz</tt>.<br>
You can sanity check your submission with this 
<A href="P2B_check.sh">test script</A>.<br>
<u>Projects that do not pass the test script will not be accepted!</u>
</p>
<p>
We will test it on a departmental Linux server.  
You would be well advised to test your submission on that platform before submitting it.
</p>

<h2> GRADING: </h2>
<P>
Points for this project will be awarded:
</P>
<div align="center">
<table><tbody>
<tr> <th>value</th>	<th align="left">feature</th></tr>

<tr> <td></td>		<th align="left">Packaging and build (10% total)</th></tr>
<tr> <td>2%</td>	<td>un-tars expected contents</td></tr>
<tr> <td>3%</td>	<td>clean build of correct programs w/default action (no warnings)</td></tr>
<tr> <td>3%</td>	<td>Makefile has working <tt>clean</tt>,
			    <tt>dist</tt>, <tt>tests</tt>, <tt>profile</tt> 
			    and <tt>graphs</tt> targets</td></tr>
<tr> <td>2%</td>	<td>reasonableness of <tt>README</tt> contents</td></tr>

<tr> <td></td> </tr>
<tr> <td></td>		<th align="left">Code Review (20%)</th></tr>
<tr> <td>4%</td>	<td>overall readability and reasonableness</td></tr>
<tr> <td>4%</td>	<td>multi-list implementation</td></tr>
<tr> <td>4%</td>	<td>thread correctly sums up the length across sub-lists</td></tr>
<tr> <td>4%</td>	<td>mutex use on multi-lists</td></tr>
<tr> <td>4%</td>	<td>spin-lock use on multi-lists</td></tr>

<tr> <td></td> </tr>
<tr> <td></td>		<th align="left">Results (40% total) ... produces correct output for</th></tr>
<tr> <td>5%</td>	<td>lists</td></tr>
<tr> <td>5%</td>	<td>correct mutex</td></tr>
<tr> <td>5%</td>	<td>correct spin</td></tr>
<tr> <td>5%</td>	<td>reasonable time per operation reporting</td></tr>
<tr> <td>5%</td>	<td>reasonable wait for mutex reporting</td></tr>
<tr> <td>10%</td>	<td>graphs (showed what we asked for)</td></tr>
<tr> <td>5%</td>	<td>profiling report (clearly shows where cycles went)</td></tr>

<tr> <td></td> </tr>
<tr> <td></td>		<th align="left">Analysis (30% total) ... reasonably explained all results in README</th></tr>
<tr> <td>2%</td>	<td>General clarity of thought and understanding</td></tr>
<tr> <td>7%</td>	<td>2.3.1 where the cycles go</td></tr>
<tr> <td>7%</td>	<td>2.3.2 profiling</td></tr>
<tr> <td>7%</td>	<td>2.3.3 wait time</td></tr>
<tr> <td>7%</td>	<td>2.3.4 list partitioning</td></tr>
</tbody></table>
</div>

<p>
Note:
 if your program does not accept the correct options or produce the 
correct output, you are likely to receive a zero for the results portion
 of your grade.  Look carefully at the sample commands and output. 
 If you have questions, ask.</p>
</body>
</html>
